import numpy as np
import pandas as pd
import re

import nltk.data
tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')
from nltk.corpus import stopwords
import gensim
from gensim.models import word2vec
from gensim import corpora, models ,similarities
from sklearn.model_selection import train_test_split



data=pd.read_csv("../input/twitter-airline-sentiment/Tweets.csv")




def get_data_ready(data):
    
    statusNum={'positive': 1 ,'neutral': 2 , 'negative': 0}

    data['status']=data['airline_sentiment'].map(statusNum)

    train,test =train_test_split(data,test_size=0.2,stratify=data['airline_sentiment'])
    
    return train,test

def get_class_counts(df):
    c=df.groupby(data['airline_sentiment'])['tweet_id'].nunique()
    return {key:c[key] for key in list(c.keys())}
    


#convert text into words sequence
def text2words(text):
    #remove ay 7aga msh letter
    text=re.sub("[^a-zA-Z]"," ",text)
    words=text.lower().split()
    return (words)

def text2sentences(text):
    tokenized_text=tokenizer.tokenize(text.strip())
    sentences=[] #hyb2a 3ndy list of lists list of goml kol gmla gwaha list of words
    for token in tokenized_text:
        if(len(token)!=0):
            sentences.append(text2words(token))
    return sentences    

train,test=get_data_ready(data)

#train.dropna()
#test.dropna()
#test.head()
#train_prob=get_class_counts(train)
#test_prob=get_class_counts(test)
#print("train",train_prob)
#print("test",test_prob)
sentences=[]
for text in train['text']:
    sentences+=text2sentences(text)
#print(sentences)    
sizeof=100
#################### building model ####################
#size equal no.of features
#sg->1 (skip diagram)
#workers =1 3shn 22dr 23ml reproducible run , to avoid scheduling threads w irritated ordering
model=word2vec.Word2Vec(sentences,workers=1,size=sizeof,min_count=40,window=5,sample=1e-3,sg=1,seed=50,alpha=0.05)
# To make the model memory efficient
model.init_sims(replace=True)
#model.wv.most_similar("bad")

###feature vector for words 

def checkIfExists(words,model):
    flag=-1
    index2word_set = set(model.wv.index2word)
    for word in  words:
        if index2word_set.__contains__(word):
            flag=1
            return flag
    return flag    

def average_words(words,model,num_features):
    #zero initialized array asra3
    featureVec = np.zeros(num_features,dtype="float32")
    nofwords=0
    atleastone=0
    index2word_set = set(model.wv.index2word)
   
    for word in  words:
        if index2word_set.__contains__(word):
            atleastone=1
            nofwords = nofwords + 1
            featureVec = np.add(featureVec,model[word])
    if(atleastone==1):
        
        featureVec = np.divide(featureVec, nofwords) 
        return featureVec,atleastone
    else:
        print("canot predict")
        return
    #print(featureVec)
    


def getTrainDataReady():
    cleanTextToWords=[]
    for text in train['text']:
        cleanTextToWords.append(text2words(text))
    flag=0
    count=0
    AvgFeatureVecs = np.zeros((len(cleanTextToWords),sizeof),dtype="float32")

    for text in cleanTextToWords:
        AvgFeatureVecs[count],flag=average_words(text,model,sizeof)
        count=count+1
    return AvgFeatureVecs

def getTestDataReady():
    cleanTextToWords=[]
    for text in test['text']:
        cleanTextToWords.append(text2words(text))
    flag=0
    count=0
    AvgFeatureVecs = np.zeros((len(cleanTextToWords),sizeof),dtype="float32")

    for text in cleanTextToWords:
        AvgFeatureVecs[count],flag=average_words(text,model,sizeof)
        count=count+1
    return AvgFeatureVecs

def getInputDataReady(inputTopredict):
    cleanTextToWords=[]
    cleanTextToWords.append(text2words(inputTopredict))
    count=0
    flag=0
    AvgFeatureVecs = np.zeros((len(cleanTextToWords),sizeof),dtype="float32")
    
    for text in cleanTextToWords:
        check=checkIfExists(text,model)
        if(check==1):
            AvgFeatureVecs[count],flag=average_words(text,model,sizeof)
            count=count+1
        
        
    return AvgFeatureVecs,flag

AvgFeatureVec=[]
AvgFeatureVec=getTrainDataReady()
testdatavec=[]
testdatavec=getTestDataReady()


from sklearn.svm import SVC
clf = SVC(decision_function_shape='ovo', C=100, gamma='auto', kernel='linear')
clf.fit(AvgFeatureVec, train['status'])

result=clf.predict(testdatavec)
output = pd.DataFrame(data={"Review":test["text"], "sentiment":result})

output.to_csv( "output.csv", index=False, quoting=3 ,escapechar="\\")
print("The accuracy is => ",clf.score(testdatavec,test['status'])*100,"%")
#print(clf.score(testdatavec,test['status'])*100 )
#print("%")

def getConsoleInput():
    MultiLine = []
    while True:
        line = input()
        if line:
            MultiLine.append(line)
        else:
            break
    finalText = '\n'.join(MultiLine)
    return finalText

def UserPrediction():
    inputvectortotest=[]
    print("Please enter your review : ")
    inputTopredict=getConsoleInput()

    print(inputTopredict)
    #check lw el input fady 3shn my3mlsh error 
    if(inputTopredict):
        inputvectortotest,flag=getInputDataReady(inputTopredict)
        
   
    #check lw mfesh ay word matching zero match 
    if( not inputTopredict or flag==0):
        print("can't predict this value")
    else:
        
        result=clf.predict(inputvectortotest)
        if(result==1):
            print("The sentiment result is : Positive ==>",result)
        elif(result==0):
            print("The sentiment result is : Negative ==>",result)
        else:
            print("The sentiment result is : Neutral ==>",result)
            
UserPrediction()